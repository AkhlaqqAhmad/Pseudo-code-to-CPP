{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s9YvyiiU2HGr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XTn23DHgxZEq",
        "outputId": "cff42430-b594-4b37-ce7e-edfbf08eb57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting dataset info...\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-a692b16dc4d5>:13: DtypeWarning: Columns (2,4,5,6) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  train_df = pd.read_csv(train_file, sep='\\t', names=['text', 'code', 'workerid', 'probid', 'subid', 'line', 'indent'])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rows in spoc-train.tsv: 272989\n",
            "Rows in spoc-testp.tsv: 52058\n",
            "Rows in spoc-testw.tsv: 34899\n",
            "Unique probid/subid combinations in spoc-train.tsv: 13454\n",
            "Split directory not found. Please provide path or confirm its absence.\n",
            "\n",
            "Sample rows from spoc-train.tsv:\n",
            "Row 203791: text='nan', code='}', workerid=54, probid=86A, subid=18134121, line=8, indent=0\n",
            "Row 250891: text='nan', code='int main() {', workerid=5, probid=630A, subid=48425743, line=0, indent=0\n",
            "Row 100883: text='ans=1', code='long long ans = 1;', workerid=31, probid=553A, subid=40995002, line=27, indent=2\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "\n",
        "# Define file paths (adjust these to your actual paths if needed)\n",
        "base_path = Path(\"./\")  # Change to your dataset directory if not current dir\n",
        "train_file = base_path / \"spoc-train.tsv\"\n",
        "testp_file = base_path / \"spoc-testp.tsv\"\n",
        "testw_file = base_path / \"spoc-testw.tsv\"\n",
        "split_dir = base_path / \"train/split\"  # For split details, if available\n",
        "\n",
        "# 1. Row Counts\n",
        "def get_row_counts():\n",
        "    train_df = pd.read_csv(train_file, sep='\\t', names=['text', 'code', 'workerid', 'probid', 'subid', 'line', 'indent'])\n",
        "    testp_df = pd.read_csv(testp_file, sep='\\t', names=['text', 'code', 'workerid', 'probid', 'subid', 'line', 'indent'])\n",
        "    testw_df = pd.read_csv(testw_file, sep='\\t', names=['text', 'code', 'workerid', 'probid', 'subid', 'line', 'indent'])\n",
        "\n",
        "    print(f\"Rows in spoc-train.tsv: {len(train_df)}\")\n",
        "    print(f\"Rows in spoc-testp.tsv: {len(testp_df)}\")\n",
        "    print(f\"Rows in spoc-testw.tsv: {len(testw_df)}\")\n",
        "    return train_df, testp_df, testw_df\n",
        "\n",
        "# 2. Unique Programs in spoc-train.tsv\n",
        "def get_unique_programs(train_df):\n",
        "    unique_programs = train_df.groupby(['probid', 'subid']).ngroups\n",
        "    print(f\"Unique probid/subid combinations in spoc-train.tsv: {unique_programs}\")\n",
        "\n",
        "# 3. Split Details (if split directory exists)\n",
        "def get_split_details():\n",
        "    if split_dir.exists():\n",
        "        split_files = list(split_dir.glob(\"*.tsv\"))\n",
        "        for split_file in split_files:\n",
        "            split_df = pd.read_csv(split_file, sep='\\t', names=['text', 'code', 'workerid', 'probid', 'subid', 'line', 'indent'])\n",
        "            print(f\"Rows in {split_file.name}: {len(split_df)}\")\n",
        "    else:\n",
        "        print(\"Split directory not found. Please provide path or confirm its absence.\")\n",
        "\n",
        "# 4. Sample Lines from spoc-train.tsv\n",
        "def get_sample_lines(train_df, num_samples=3):\n",
        "    samples = train_df.sample(n=num_samples)\n",
        "    print(\"\\nSample rows from spoc-train.tsv:\")\n",
        "    for idx, row in samples.iterrows():\n",
        "        print(f\"Row {idx}: text='{row['text']}', code='{row['code']}', workerid={row['workerid']}, \"\n",
        "              f\"probid={row['probid']}, subid={row['subid']}, line={row['line']}, indent={row['indent']}\")\n",
        "\n",
        "# Run everything\n",
        "print(\"Extracting dataset info...\\n\")\n",
        "train_df, testp_df, testw_df = get_row_counts()\n",
        "get_unique_programs(train_df)\n",
        "get_split_details()\n",
        "get_sample_lines(train_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "from collections import Counter\n",
        "from pathlib import Path\n",
        "import re\n",
        "\n",
        "torch.manual_seed(42)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "base_path = Path(\"./\")\n",
        "train_file = base_path / \"spoc-train.tsv\"\n",
        "\n",
        "def load_spoc_tsv(file_path):\n",
        "    df = pd.read_csv(file_path, sep='\\t', names=['text', 'code', 'workerid', 'probid', 'subid', 'line', 'indent'],\n",
        "                     low_memory=False, skiprows=1)\n",
        "    df['text'] = df['text'].fillna('')\n",
        "    print(f\"Loaded {len(df)} rows from {file_path}\")\n",
        "    return df\n",
        "\n",
        "def group_into_programs(df):\n",
        "    programs = []\n",
        "    grouped = df.groupby(['probid', 'subid'])\n",
        "    for (probid, subid), group in grouped:\n",
        "        group = group.sort_values('line')\n",
        "        pseudo_lines = group['text'].tolist()  # Pseudocode as source\n",
        "        cpp_lines = group['code'].tolist()     # C++ as target\n",
        "        indents = group['indent'].astype(int).tolist()\n",
        "        pseudo_program = '\\n'.join(f\"{'  ' * indent}{line}\" for indent, line in zip(indents, pseudo_lines))\n",
        "        cpp_program = '\\n'.join(f\"{'  ' * indent}{line}\" for indent, line in zip(indents, cpp_lines))\n",
        "        programs.append((pseudo_program, cpp_program))  # (src, tgt)\n",
        "    print(f\"Grouped into {len(programs)} full programs\")\n",
        "    return programs\n",
        "\n",
        "def tokenize(text):\n",
        "    tokens = re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
        "    return tokens\n",
        "\n",
        "def build_vocab(programs, min_freq=2):\n",
        "    pseudo_counter = Counter()  # Source\n",
        "    cpp_counter = Counter()     # Target\n",
        "    for pseudo, cpp in programs:\n",
        "        pseudo_counter.update(tokenize(pseudo))\n",
        "        cpp_counter.update(tokenize(cpp))\n",
        "    specials = ['<pad>', '<sos>', '<eos>', '<unk>']\n",
        "    pseudo_vocab = specials + [word for word, freq in pseudo_counter.items() if freq >= min_freq]\n",
        "    cpp_vocab = specials + [word for word, freq in cpp_counter.items() if freq >= min_freq]\n",
        "    pseudo2idx = {word: idx for idx, word in enumerate(pseudo_vocab)}\n",
        "    idx2pseudo = {idx: word for word, idx in pseudo2idx.items()}\n",
        "    cpp2idx = {word: idx for idx, word in enumerate(cpp_vocab)}\n",
        "    idx2cpp = {idx: word for word, idx in cpp2idx.items()}\n",
        "    print(f\"Pseudocode vocab size: {len(pseudo_vocab)}, C++ vocab size: {len(cpp_vocab)}\")\n",
        "    return pseudo2idx, idx2pseudo, cpp2idx, idx2cpp\n",
        "\n",
        "def text_to_tensor(text, vocab, tokenizer, max_len=200):\n",
        "    tokens = ['<sos>'] + tokenizer(text)[:max_len - 2] + ['<eos>']\n",
        "    tensor = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "    return torch.tensor(tensor, dtype=torch.long)\n",
        "\n",
        "def get_batches(programs, batch_size=32, max_len=200):\n",
        "    for i in range(0, len(programs), batch_size):\n",
        "        batch = programs[i:i + batch_size]\n",
        "        src_batch = [text_to_tensor(pseudo, pseudo2idx, tokenize, max_len) for pseudo, _ in batch]\n",
        "        tgt_batch = [text_to_tensor(cpp, cpp2idx, tokenize, max_len) for _, cpp in batch]\n",
        "        src_batch = torch.nn.utils.rnn.pad_sequence(src_batch, padding_value=pseudo2idx['<pad>'], batch_first=True).to(device)\n",
        "        tgt_batch = torch.nn.utils.rnn.pad_sequence(tgt_batch, padding_value=cpp2idx['<pad>'], batch_first=True).to(device)\n",
        "        yield src_batch, tgt_batch\n",
        "\n",
        "train_df = load_spoc_tsv(train_file)\n",
        "train_programs = group_into_programs(train_df)\n",
        "pseudo2idx, idx2pseudo, cpp2idx, idx2cpp = build_vocab(train_programs)\n",
        "torch.save(pseudo2idx, \"pseudo2idx.pt\")\n",
        "torch.save(idx2pseudo, \"idx2pseudo.pt\")\n",
        "torch.save(cpp2idx, \"cpp2idx.pt\")\n",
        "torch.save(idx2cpp, \"idx2cpp.pt\")\n",
        "print(\"Chunk 1 completed: Data preprocessed and vocab saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTiNc0mL4Eno",
        "outputId": "1cf9bad5-c76f-4695-ea88-aaf7b635c65f"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 293854 rows from spoc-train.tsv\n",
            "Grouped into 14548 full programs\n",
            "Pseudocode vocab size: 6217, C++ vocab size: 5643\n",
            "Chunk 1 completed: Data preprocessed and vocab saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "import torch.nn as nn\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=200):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        return torch.matmul(attn, V)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.W_o(output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.ff(x)\n",
        "        return self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        attn1_output = self.mha1(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn1_output))\n",
        "        attn2_output = self.mha2(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn2_output))\n",
        "        ff_output = self.ff(x)\n",
        "        return self.norm3(x + self.dropout(ff_output))\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, num_heads=8, num_layers=3, d_ff=1024, max_len=200, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != pseudo2idx['<pad>']).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != cpp2idx['<pad>']).unsqueeze(1).unsqueeze(3)\n",
        "        seq_len = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1)).bool().to(device)\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.pos_encoding(self.src_embedding(src) * math.sqrt(self.d_model)))\n",
        "        tgt_embedded = self.dropout(self.pos_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model)))\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "        return self.fc_out(dec_output)\n",
        "\n",
        "print(\"Chunk 2 completed: Transformer model defined.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0j0z9ify6mG6",
        "outputId": "df10cd01-1264-4deb-844e-13dd2c54a31a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 2 completed: Transformer model defined.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "model = Transformer(\n",
        "    src_vocab_size=len(pseudo2idx),\n",
        "    tgt_vocab_size=len(cpp2idx),\n",
        "    d_model=256,\n",
        "    num_heads=8,\n",
        "    num_layers=3,\n",
        "    d_ff=1024,\n",
        "    max_len=200,\n",
        "    dropout=0.1\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=cpp2idx['<pad>'])\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
        "\n",
        "def train(model, programs, epochs=35, batch_size=32):  # Increased to 50 epochs\n",
        "    model.train()\n",
        "    total_batches = len(programs) // batch_size + (1 if len(programs) % batch_size else 0)\n",
        "    print(f\"Total batches per epoch: {total_batches}\")\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for batch_idx, (src_batch, tgt_batch) in enumerate(get_batches(programs, batch_size), 1):\n",
        "            if batch_idx % 50 == 0:\n",
        "                print(f\"Epoch {epoch + 1}, Batch {batch_idx}/{total_batches}\")\n",
        "            optimizer.zero_grad()\n",
        "            output = model(src_batch, tgt_batch[:, :-1])\n",
        "            loss = criterion(output.reshape(-1, len(cpp2idx)), tgt_batch[:, 1:].reshape(-1))\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        avg_loss = total_loss / total_batches\n",
        "        print(f\"Epoch {epoch + 1}/{epochs}, Loss: {avg_loss:.4f}\")\n",
        "    torch.save(model.state_dict(), \"pseudo2cpp_model.pt\")\n",
        "    print(\"Model saved to pseudo2cpp_model.pt\")\n",
        "\n",
        "def translate(model, pseudo_program, max_len=200):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        src = text_to_tensor(pseudo_program, pseudo2idx, tokenize, max_len).unsqueeze(0).to(device)\n",
        "        tgt = torch.tensor([cpp2idx['<sos>']], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        for _ in range(max_len):\n",
        "            output = model(src, tgt)\n",
        "            next_token = output[:, -1, :].argmax(dim=-1).item()\n",
        "            if next_token == cpp2idx['<eos>']:\n",
        "                break\n",
        "            tgt = torch.cat([tgt, torch.tensor([[next_token]], dtype=torch.long).to(device)], dim=1)\n",
        "        translated = [idx2cpp[idx.item()] for idx in tgt[0] if idx.item() in idx2cpp]\n",
        "        cpp = ' '.join(translated[1:])\n",
        "        # Improved post-processing for C++ syntax\n",
        "        cpp = cpp.replace('int main', '#include <iostream>\\nusing namespace std;\\n\\nint main')\n",
        "        cpp = re.sub(r'(\\bint\\b|\\bfor\\b|\\bwhile\\b|\\bif\\b|\\belse\\b|\\{|\\})', r'\\n\\1', cpp)\n",
        "        cpp = re.sub(r'input (\\w+)', r'cin >> \\1;', cpp)\n",
        "        cpp = re.sub(r'mod', '%', cpp)\n",
        "        cpp = re.sub(r'(\\w+) = (\\w+)', r'\\1 == \\2', cpp, count=1)  # Fix first '=' to '==' in 'if'\n",
        "        cpp = re.sub(r'(\\w+) = ', r'\\1 = ', cpp)  # Preserve other assignments\n",
        "        cpp = re.sub(r'to', '<=', cpp)\n",
        "        cpp = re.sub(r'then', '', cpp)\n",
        "        lines = cpp.split('\\n')\n",
        "        indented = []\n",
        "        indent_level = 0\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                if '}' in line:\n",
        "                    indent_level = max(0, indent_level - 1)\n",
        "                indented.append('  ' * indent_level + line + (';' if line[-1] not in '{;}' else ''))\n",
        "                if '{' in line and '}' not in line:\n",
        "                    indent_level += 1\n",
        "        return '\\n'.join(indented).strip()\n",
        "\n",
        "print(\"Starting training...\")\n",
        "train(model, train_programs, epochs=35)  # ~10-20h GPU\n",
        "print(\"Training completed.\")\n",
        "\n",
        "test_pseudo = \"\"\"\n",
        "declare gcd(a, b)\n",
        "  if b = 0 then return a\n",
        "  else return gcd(b, a mod b)\n",
        "declare main\n",
        "  declare n, nn, ans = 0\n",
        "  input n\n",
        "  for i = 2 to n - 1\n",
        "    nn = n\n",
        "    while nn > 0\n",
        "      ans = ans + (nn mod i)\n",
        "      nn = nn / i\n",
        "\"\"\"\n",
        "translated = translate(model, test_pseudo)\n",
        "print(f\"Input Pseudocode:\\n{test_pseudo}\")\n",
        "print(f\"Translated C++:\\n{translated}\")\n",
        "\n",
        "print(\"Chunk 3 completed: Model trained and saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD-qrmt96oiP",
        "outputId": "71b8b6cd-75ee-4067-c880-2ce8e1db5c60"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting training...\n",
            "Total batches per epoch: 455\n",
            "Epoch 1, Batch 50/455\n",
            "Epoch 1, Batch 100/455\n",
            "Epoch 1, Batch 150/455\n",
            "Epoch 1, Batch 200/455\n",
            "Epoch 1, Batch 250/455\n",
            "Epoch 1, Batch 300/455\n",
            "Epoch 1, Batch 350/455\n",
            "Epoch 1, Batch 400/455\n",
            "Epoch 1, Batch 450/455\n",
            "Epoch 1/35, Loss: 3.3352\n",
            "Epoch 2, Batch 50/455\n",
            "Epoch 2, Batch 100/455\n",
            "Epoch 2, Batch 150/455\n",
            "Epoch 2, Batch 200/455\n",
            "Epoch 2, Batch 250/455\n",
            "Epoch 2, Batch 300/455\n",
            "Epoch 2, Batch 350/455\n",
            "Epoch 2, Batch 400/455\n",
            "Epoch 2, Batch 450/455\n",
            "Epoch 2/35, Loss: 2.1442\n",
            "Epoch 3, Batch 50/455\n",
            "Epoch 3, Batch 100/455\n",
            "Epoch 3, Batch 150/455\n",
            "Epoch 3, Batch 200/455\n",
            "Epoch 3, Batch 250/455\n",
            "Epoch 3, Batch 300/455\n",
            "Epoch 3, Batch 350/455\n",
            "Epoch 3, Batch 400/455\n",
            "Epoch 3, Batch 450/455\n",
            "Epoch 3/35, Loss: 1.8764\n",
            "Epoch 4, Batch 50/455\n",
            "Epoch 4, Batch 100/455\n",
            "Epoch 4, Batch 150/455\n",
            "Epoch 4, Batch 200/455\n",
            "Epoch 4, Batch 250/455\n",
            "Epoch 4, Batch 300/455\n",
            "Epoch 4, Batch 350/455\n",
            "Epoch 4, Batch 400/455\n",
            "Epoch 4, Batch 450/455\n",
            "Epoch 4/35, Loss: 1.7056\n",
            "Epoch 5, Batch 50/455\n",
            "Epoch 5, Batch 100/455\n",
            "Epoch 5, Batch 150/455\n",
            "Epoch 5, Batch 200/455\n",
            "Epoch 5, Batch 250/455\n",
            "Epoch 5, Batch 300/455\n",
            "Epoch 5, Batch 350/455\n",
            "Epoch 5, Batch 400/455\n",
            "Epoch 5, Batch 450/455\n",
            "Epoch 5/35, Loss: 1.5742\n",
            "Epoch 6, Batch 50/455\n",
            "Epoch 6, Batch 100/455\n",
            "Epoch 6, Batch 150/455\n",
            "Epoch 6, Batch 200/455\n",
            "Epoch 6, Batch 250/455\n",
            "Epoch 6, Batch 300/455\n",
            "Epoch 6, Batch 350/455\n",
            "Epoch 6, Batch 400/455\n",
            "Epoch 6, Batch 450/455\n",
            "Epoch 6/35, Loss: 1.4713\n",
            "Epoch 7, Batch 50/455\n",
            "Epoch 7, Batch 100/455\n",
            "Epoch 7, Batch 150/455\n",
            "Epoch 7, Batch 200/455\n",
            "Epoch 7, Batch 250/455\n",
            "Epoch 7, Batch 300/455\n",
            "Epoch 7, Batch 350/455\n",
            "Epoch 7, Batch 400/455\n",
            "Epoch 7, Batch 450/455\n",
            "Epoch 7/35, Loss: 1.3915\n",
            "Epoch 8, Batch 50/455\n",
            "Epoch 8, Batch 100/455\n",
            "Epoch 8, Batch 150/455\n",
            "Epoch 8, Batch 200/455\n",
            "Epoch 8, Batch 250/455\n",
            "Epoch 8, Batch 300/455\n",
            "Epoch 8, Batch 350/455\n",
            "Epoch 8, Batch 400/455\n",
            "Epoch 8, Batch 450/455\n",
            "Epoch 8/35, Loss: 1.3254\n",
            "Epoch 9, Batch 50/455\n",
            "Epoch 9, Batch 100/455\n",
            "Epoch 9, Batch 150/455\n",
            "Epoch 9, Batch 200/455\n",
            "Epoch 9, Batch 250/455\n",
            "Epoch 9, Batch 300/455\n",
            "Epoch 9, Batch 350/455\n",
            "Epoch 9, Batch 400/455\n",
            "Epoch 9, Batch 450/455\n",
            "Epoch 9/35, Loss: 1.2711\n",
            "Epoch 10, Batch 50/455\n",
            "Epoch 10, Batch 100/455\n",
            "Epoch 10, Batch 150/455\n",
            "Epoch 10, Batch 200/455\n",
            "Epoch 10, Batch 250/455\n",
            "Epoch 10, Batch 300/455\n",
            "Epoch 10, Batch 350/455\n",
            "Epoch 10, Batch 400/455\n",
            "Epoch 10, Batch 450/455\n",
            "Epoch 10/35, Loss: 1.2200\n",
            "Epoch 11, Batch 50/455\n",
            "Epoch 11, Batch 100/455\n",
            "Epoch 11, Batch 150/455\n",
            "Epoch 11, Batch 200/455\n",
            "Epoch 11, Batch 250/455\n",
            "Epoch 11, Batch 300/455\n",
            "Epoch 11, Batch 350/455\n",
            "Epoch 11, Batch 400/455\n",
            "Epoch 11, Batch 450/455\n",
            "Epoch 11/35, Loss: 1.1760\n",
            "Epoch 12, Batch 50/455\n",
            "Epoch 12, Batch 100/455\n",
            "Epoch 12, Batch 150/455\n",
            "Epoch 12, Batch 200/455\n",
            "Epoch 12, Batch 250/455\n",
            "Epoch 12, Batch 300/455\n",
            "Epoch 12, Batch 350/455\n",
            "Epoch 12, Batch 400/455\n",
            "Epoch 12, Batch 450/455\n",
            "Epoch 12/35, Loss: 1.1322\n",
            "Epoch 13, Batch 50/455\n",
            "Epoch 13, Batch 100/455\n",
            "Epoch 13, Batch 150/455\n",
            "Epoch 13, Batch 200/455\n",
            "Epoch 13, Batch 250/455\n",
            "Epoch 13, Batch 300/455\n",
            "Epoch 13, Batch 350/455\n",
            "Epoch 13, Batch 400/455\n",
            "Epoch 13, Batch 450/455\n",
            "Epoch 13/35, Loss: 1.0968\n",
            "Epoch 14, Batch 50/455\n",
            "Epoch 14, Batch 100/455\n",
            "Epoch 14, Batch 150/455\n",
            "Epoch 14, Batch 200/455\n",
            "Epoch 14, Batch 250/455\n",
            "Epoch 14, Batch 300/455\n",
            "Epoch 14, Batch 350/455\n",
            "Epoch 14, Batch 400/455\n",
            "Epoch 14, Batch 450/455\n",
            "Epoch 14/35, Loss: 1.0699\n",
            "Epoch 15, Batch 50/455\n",
            "Epoch 15, Batch 100/455\n",
            "Epoch 15, Batch 150/455\n",
            "Epoch 15, Batch 200/455\n",
            "Epoch 15, Batch 250/455\n",
            "Epoch 15, Batch 300/455\n",
            "Epoch 15, Batch 350/455\n",
            "Epoch 15, Batch 400/455\n",
            "Epoch 15, Batch 450/455\n",
            "Epoch 15/35, Loss: 1.0398\n",
            "Epoch 16, Batch 50/455\n",
            "Epoch 16, Batch 100/455\n",
            "Epoch 16, Batch 150/455\n",
            "Epoch 16, Batch 200/455\n",
            "Epoch 16, Batch 250/455\n",
            "Epoch 16, Batch 300/455\n",
            "Epoch 16, Batch 350/455\n",
            "Epoch 16, Batch 400/455\n",
            "Epoch 16, Batch 450/455\n",
            "Epoch 16/35, Loss: 1.0124\n",
            "Epoch 17, Batch 50/455\n",
            "Epoch 17, Batch 100/455\n",
            "Epoch 17, Batch 150/455\n",
            "Epoch 17, Batch 200/455\n",
            "Epoch 17, Batch 250/455\n",
            "Epoch 17, Batch 300/455\n",
            "Epoch 17, Batch 350/455\n",
            "Epoch 17, Batch 400/455\n",
            "Epoch 17, Batch 450/455\n",
            "Epoch 17/35, Loss: 0.9809\n",
            "Epoch 18, Batch 50/455\n",
            "Epoch 18, Batch 100/455\n",
            "Epoch 18, Batch 150/455\n",
            "Epoch 18, Batch 200/455\n",
            "Epoch 18, Batch 250/455\n",
            "Epoch 18, Batch 300/455\n",
            "Epoch 18, Batch 350/455\n",
            "Epoch 18, Batch 400/455\n",
            "Epoch 18, Batch 450/455\n",
            "Epoch 18/35, Loss: 0.9568\n",
            "Epoch 19, Batch 50/455\n",
            "Epoch 19, Batch 100/455\n",
            "Epoch 19, Batch 150/455\n",
            "Epoch 19, Batch 200/455\n",
            "Epoch 19, Batch 250/455\n",
            "Epoch 19, Batch 300/455\n",
            "Epoch 19, Batch 350/455\n",
            "Epoch 19, Batch 400/455\n",
            "Epoch 19, Batch 450/455\n",
            "Epoch 19/35, Loss: 0.9339\n",
            "Epoch 20, Batch 50/455\n",
            "Epoch 20, Batch 100/455\n",
            "Epoch 20, Batch 150/455\n",
            "Epoch 20, Batch 200/455\n",
            "Epoch 20, Batch 250/455\n",
            "Epoch 20, Batch 300/455\n",
            "Epoch 20, Batch 350/455\n",
            "Epoch 20, Batch 400/455\n",
            "Epoch 20, Batch 450/455\n",
            "Epoch 20/35, Loss: 0.9121\n",
            "Epoch 21, Batch 50/455\n",
            "Epoch 21, Batch 100/455\n",
            "Epoch 21, Batch 150/455\n",
            "Epoch 21, Batch 200/455\n",
            "Epoch 21, Batch 250/455\n",
            "Epoch 21, Batch 300/455\n",
            "Epoch 21, Batch 350/455\n",
            "Epoch 21, Batch 400/455\n",
            "Epoch 21, Batch 450/455\n",
            "Epoch 21/35, Loss: 0.8927\n",
            "Epoch 22, Batch 50/455\n",
            "Epoch 22, Batch 100/455\n",
            "Epoch 22, Batch 150/455\n",
            "Epoch 22, Batch 200/455\n",
            "Epoch 22, Batch 250/455\n",
            "Epoch 22, Batch 300/455\n",
            "Epoch 22, Batch 350/455\n",
            "Epoch 22, Batch 400/455\n",
            "Epoch 22, Batch 450/455\n",
            "Epoch 22/35, Loss: 0.8726\n",
            "Epoch 23, Batch 50/455\n",
            "Epoch 23, Batch 100/455\n",
            "Epoch 23, Batch 150/455\n",
            "Epoch 23, Batch 200/455\n",
            "Epoch 23, Batch 250/455\n",
            "Epoch 23, Batch 300/455\n",
            "Epoch 23, Batch 350/455\n",
            "Epoch 23, Batch 400/455\n",
            "Epoch 23, Batch 450/455\n",
            "Epoch 23/35, Loss: 0.8535\n",
            "Epoch 24, Batch 50/455\n",
            "Epoch 24, Batch 100/455\n",
            "Epoch 24, Batch 150/455\n",
            "Epoch 24, Batch 200/455\n",
            "Epoch 24, Batch 250/455\n",
            "Epoch 24, Batch 300/455\n",
            "Epoch 24, Batch 350/455\n",
            "Epoch 24, Batch 400/455\n",
            "Epoch 24, Batch 450/455\n",
            "Epoch 24/35, Loss: 0.8362\n",
            "Epoch 25, Batch 50/455\n",
            "Epoch 25, Batch 100/455\n",
            "Epoch 25, Batch 150/455\n",
            "Epoch 25, Batch 200/455\n",
            "Epoch 25, Batch 250/455\n",
            "Epoch 25, Batch 300/455\n",
            "Epoch 25, Batch 350/455\n",
            "Epoch 25, Batch 400/455\n",
            "Epoch 25, Batch 450/455\n",
            "Epoch 25/35, Loss: 0.8186\n",
            "Epoch 26, Batch 50/455\n",
            "Epoch 26, Batch 100/455\n",
            "Epoch 26, Batch 150/455\n",
            "Epoch 26, Batch 200/455\n",
            "Epoch 26, Batch 250/455\n",
            "Epoch 26, Batch 300/455\n",
            "Epoch 26, Batch 350/455\n",
            "Epoch 26, Batch 400/455\n",
            "Epoch 26, Batch 450/455\n",
            "Epoch 26/35, Loss: 0.8062\n",
            "Epoch 27, Batch 50/455\n",
            "Epoch 27, Batch 100/455\n",
            "Epoch 27, Batch 150/455\n",
            "Epoch 27, Batch 200/455\n",
            "Epoch 27, Batch 250/455\n",
            "Epoch 27, Batch 300/455\n",
            "Epoch 27, Batch 350/455\n",
            "Epoch 27, Batch 400/455\n",
            "Epoch 27, Batch 450/455\n",
            "Epoch 27/35, Loss: 0.7982\n",
            "Epoch 28, Batch 50/455\n",
            "Epoch 28, Batch 100/455\n",
            "Epoch 28, Batch 150/455\n",
            "Epoch 28, Batch 200/455\n",
            "Epoch 28, Batch 250/455\n",
            "Epoch 28, Batch 300/455\n",
            "Epoch 28, Batch 350/455\n",
            "Epoch 28, Batch 400/455\n",
            "Epoch 28, Batch 450/455\n",
            "Epoch 28/35, Loss: 0.7776\n",
            "Epoch 29, Batch 50/455\n",
            "Epoch 29, Batch 100/455\n",
            "Epoch 29, Batch 150/455\n",
            "Epoch 29, Batch 200/455\n",
            "Epoch 29, Batch 250/455\n",
            "Epoch 29, Batch 300/455\n",
            "Epoch 29, Batch 350/455\n",
            "Epoch 29, Batch 400/455\n",
            "Epoch 29, Batch 450/455\n",
            "Epoch 29/35, Loss: 0.7626\n",
            "Epoch 30, Batch 50/455\n",
            "Epoch 30, Batch 100/455\n",
            "Epoch 30, Batch 150/455\n",
            "Epoch 30, Batch 200/455\n",
            "Epoch 30, Batch 250/455\n",
            "Epoch 30, Batch 300/455\n",
            "Epoch 30, Batch 350/455\n",
            "Epoch 30, Batch 400/455\n",
            "Epoch 30, Batch 450/455\n",
            "Epoch 30/35, Loss: 0.7488\n",
            "Epoch 31, Batch 50/455\n",
            "Epoch 31, Batch 100/455\n",
            "Epoch 31, Batch 150/455\n",
            "Epoch 31, Batch 200/455\n",
            "Epoch 31, Batch 250/455\n",
            "Epoch 31, Batch 300/455\n",
            "Epoch 31, Batch 350/455\n",
            "Epoch 31, Batch 400/455\n",
            "Epoch 31, Batch 450/455\n",
            "Epoch 31/35, Loss: 0.7365\n",
            "Epoch 32, Batch 50/455\n",
            "Epoch 32, Batch 100/455\n",
            "Epoch 32, Batch 150/455\n",
            "Epoch 32, Batch 200/455\n",
            "Epoch 32, Batch 250/455\n",
            "Epoch 32, Batch 300/455\n",
            "Epoch 32, Batch 350/455\n",
            "Epoch 32, Batch 400/455\n",
            "Epoch 32, Batch 450/455\n",
            "Epoch 32/35, Loss: 0.7233\n",
            "Epoch 33, Batch 50/455\n",
            "Epoch 33, Batch 100/455\n",
            "Epoch 33, Batch 150/455\n",
            "Epoch 33, Batch 200/455\n",
            "Epoch 33, Batch 250/455\n",
            "Epoch 33, Batch 300/455\n",
            "Epoch 33, Batch 350/455\n",
            "Epoch 33, Batch 400/455\n",
            "Epoch 33, Batch 450/455\n",
            "Epoch 33/35, Loss: 0.7184\n",
            "Epoch 34, Batch 50/455\n",
            "Epoch 34, Batch 100/455\n",
            "Epoch 34, Batch 150/455\n",
            "Epoch 34, Batch 200/455\n",
            "Epoch 34, Batch 250/455\n",
            "Epoch 34, Batch 300/455\n",
            "Epoch 34, Batch 350/455\n",
            "Epoch 34, Batch 400/455\n",
            "Epoch 34, Batch 450/455\n",
            "Epoch 34/35, Loss: 0.7093\n",
            "Epoch 35, Batch 50/455\n",
            "Epoch 35, Batch 100/455\n",
            "Epoch 35, Batch 150/455\n",
            "Epoch 35, Batch 200/455\n",
            "Epoch 35, Batch 250/455\n",
            "Epoch 35, Batch 300/455\n",
            "Epoch 35, Batch 350/455\n",
            "Epoch 35, Batch 400/455\n",
            "Epoch 35, Batch 450/455\n",
            "Epoch 35/35, Loss: 0.6941\n",
            "Model saved to pseudo2cpp_model.pt\n",
            "Training completed.\n",
            "Input Pseudocode:\n",
            "\n",
            "declare gcd(a, b)\n",
            "  if b = 0 then return a\n",
            "  else return gcd(b, a mod b)\n",
            "declare main\n",
            "  declare n, nn, ans = 0\n",
            "  input n\n",
            "  for i = 2 to n - 1\n",
            "    nn = n\n",
            "    while nn > 0\n",
            "      ans = ans + (nn mod i)\n",
            "      nn = nn / i\n",
            "\n",
            "Translated C++:\n",
            "int gcd (;\n",
            "int a ,;\n",
            "int b );\n",
            "{\n",
            "  if ( b > 0 ) return a ;\n",
            "  else return gcd ( b , a % b ) ;\n",
            "} #include <iostream>;\n",
            "using namespace std;\n",
            "int main ( );\n",
            "{\n",
            "  int n , nn ;\n",
            "  while ( cin > > a > > b );\n",
            "  {\n",
            "    int nn == 0 , nn = 0 ;\n",
            "    for (;\n",
            "    int i = 0 ; i < n ; i + + );\n",
            "    {\n",
            "      if ( nn > = n ) ans = n / 2 ;\n",
            "      else ans = n - 1 ;\n",
            "    } cout < < ans < < endl ;\n",
            "  } return 0 ;\n",
            "}\n",
            "Chunk 3 completed: Model trained and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test complex pseudocode lines\n",
        "test_lines = [\n",
        "    \"in the function gcd(a,b=integers)\",\n",
        "    \"if b=1 return a, else call function gcd(b, a%b)\",\n",
        "    \"n , nn, ans = integers with ans =0\",\n",
        "    \"Read n\",\n",
        "    \"for i=2 to n-1 execute\",\n",
        "    \"set nn to n\",\n",
        "    \"while nn is not equal to 0, set ans to ans + nn%i, and also set nn= nn/i\"\n",
        "]\n",
        "\n",
        "print(\"Testing complex pseudocode lines:\")\n",
        "for pseudo in test_lines:\n",
        "    cpp = translate(model, pseudo)\n",
        "    print(f\"Input: {pseudo}\")\n",
        "    print(f\"Translated: {cpp}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZZHGkWktCmZl",
        "outputId": "b5bfd952-b194-454f-b883-8903a930523c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing complex pseudocode lines:\n",
            "Input: in the function gcd(a,b=integers)\n",
            "Translated: void <unk> {\n",
            "\n",
            "Input: if b=1 return a, else call function gcd(b, a%b)\n",
            "Translated: return gcd(b, a < b ? gcd(b, a : gcd(b, a - b);\n",
            "\n",
            "Input: n , nn, ans = integers with ans =0\n",
            "Translated: int n, ans = 0;\n",
            "\n",
            "Input: Read n\n",
            "Translated: cin >> n;\n",
            "\n",
            "Input: for i=2 to n-1 execute\n",
            "Translated: for (int i = 2; i <= n - 2; i++) {\n",
            "\n",
            "Input: set nn to n\n",
            "Translated: nn = n;\n",
            "\n",
            "Input: while nn is not equal to 0, set ans to ans + nn%i, and also set nn= nn/i\n",
            "Translated: while <unk> != 0) { ans = <unk> + <unk> }\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gradio"
      ],
      "metadata": {
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wp2ZqBUvanpx",
        "outputId": "c20e7098-c3dc-4f0b-bcc8-207e5e226545"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gradio\n",
            "  Downloading gradio-5.20.1-py3-none-any.whl.metadata (16 kB)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.11-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.2 (from gradio)\n",
            "  Downloading gradio_client-1.7.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting groovy~=0.1 (from gradio)\n",
            "  Downloading groovy-0.1.2-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.1.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.46.1-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.2)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.2->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.20.1-py3-none-any.whl (62.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 MB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.2-py3-none-any.whl (322 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.1/322.1 kB\u001b[0m \u001b[31m29.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.11-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.9/94.9 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading groovy-0.1.2-py3-none-any.whl (14 kB)\n",
            "Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.10-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m114.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.46.1-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, groovy, ffmpy, aiofiles, starlette, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.11 ffmpy-0.5.0 gradio-5.20.1 gradio-client-1.7.2 groovy-0.1.2 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.10 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.46.1 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import torch\n",
        "import re\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load vocab\n",
        "pseudo2idx = torch.load(\"pseudo2idx.pt\")\n",
        "idx2pseudo = torch.load(\"idx2pseudo.pt\")\n",
        "cpp2idx = torch.load(\"cpp2idx.pt\")\n",
        "idx2cpp = torch.load(\"idx2cpp.pt\")\n",
        "\n",
        "def tokenize(text):\n",
        "    return re.findall(r'\\w+|[^\\w\\s]', text, re.UNICODE)\n",
        "\n",
        "def text_to_tensor(text, vocab, tokenizer, max_len=200):\n",
        "    tokens = ['<sos>'] + tokenizer(text)[:max_len - 2] + ['<eos>']\n",
        "    tensor = [vocab.get(token, vocab['<unk>']) for token in tokens]\n",
        "    return torch.tensor(tensor, dtype=torch.long)\n",
        "\n",
        "# Transformer model (from Chunk 2)\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=200):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        pe = pe.unsqueeze(0)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :]\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model, num_heads):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0\n",
        "        self.d_model = d_model\n",
        "        self.num_heads = num_heads\n",
        "        self.d_k = d_model // num_heads\n",
        "        self.W_q = nn.Linear(d_model, d_model)\n",
        "        self.W_k = nn.Linear(d_model, d_model)\n",
        "        self.W_v = nn.Linear(d_model, d_model)\n",
        "        self.W_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "    def scaled_dot_product_attention(self, Q, K, V, mask=None):\n",
        "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
        "        if mask is not None:\n",
        "            scores = scores.masked_fill(mask == 0, -1e9)\n",
        "        attn = torch.softmax(scores, dim=-1)\n",
        "        return torch.matmul(attn, V)\n",
        "\n",
        "    def forward(self, Q, K, V, mask=None):\n",
        "        batch_size = Q.size(0)\n",
        "        Q = self.W_q(Q).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        K = self.W_k(K).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        V = self.W_v(V).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
        "        output = self.scaled_dot_product_attention(Q, K, V, mask)\n",
        "        output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
        "        return self.W_o(output)\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, d_model, d_ff):\n",
        "        super().__init__()\n",
        "        self.linear1 = nn.Linear(d_model, d_ff)\n",
        "        self.linear2 = nn.Linear(d_ff, d_model)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear2(self.relu(self.linear1(x)))\n",
        "\n",
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attn_output = self.mha(x, x, x, mask)\n",
        "        x = self.norm1(x + self.dropout(attn_output))\n",
        "        ff_output = self.ff(x)\n",
        "        return self.norm2(x + self.dropout(ff_output))\n",
        "\n",
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_ff, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
        "        self.ff = FeedForward(d_model, d_ff)\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.norm3 = nn.LayerNorm(d_model)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, enc_output, src_mask=None, tgt_mask=None):\n",
        "        attn1_output = self.mha1(x, x, x, tgt_mask)\n",
        "        x = self.norm1(x + self.dropout(attn1_output))\n",
        "        attn2_output = self.mha2(x, enc_output, enc_output, src_mask)\n",
        "        x = self.norm2(x + self.dropout(attn2_output))\n",
        "        ff_output = self.ff(x)\n",
        "        return self.norm3(x + self.dropout(ff_output))\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, src_vocab_size, tgt_vocab_size, d_model=256, num_heads=8, num_layers=3, d_ff=1024, max_len=200, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.src_embedding = nn.Embedding(src_vocab_size, d_model)\n",
        "        self.tgt_embedding = nn.Embedding(tgt_vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
        "        self.encoder_layers = nn.ModuleList([EncoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.decoder_layers = nn.ModuleList([DecoderLayer(d_model, num_heads, d_ff, dropout) for _ in range(num_layers)])\n",
        "        self.fc_out = nn.Linear(d_model, tgt_vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def generate_mask(self, src, tgt):\n",
        "        src_mask = (src != pseudo2idx['<pad>']).unsqueeze(1).unsqueeze(2)\n",
        "        tgt_mask = (tgt != cpp2idx['<pad>']).unsqueeze(1).unsqueeze(3)\n",
        "        seq_len = tgt.size(1)\n",
        "        nopeak_mask = (1 - torch.triu(torch.ones(1, seq_len, seq_len), diagonal=1)).bool().to(device)\n",
        "        tgt_mask = tgt_mask & nopeak_mask\n",
        "        return src_mask, tgt_mask\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_mask, tgt_mask = self.generate_mask(src, tgt)\n",
        "        src_embedded = self.dropout(self.pos_encoding(self.src_embedding(src) * math.sqrt(self.d_model)))\n",
        "        tgt_embedded = self.dropout(self.pos_encoding(self.tgt_embedding(tgt) * math.sqrt(self.d_model)))\n",
        "        enc_output = src_embedded\n",
        "        for enc_layer in self.encoder_layers:\n",
        "            enc_output = enc_layer(enc_output, src_mask)\n",
        "        dec_output = tgt_embedded\n",
        "        for dec_layer in self.decoder_layers:\n",
        "            dec_output = dec_layer(dec_output, enc_output, src_mask, tgt_mask)\n",
        "        return self.fc_out(dec_output)\n",
        "\n",
        "# Load model\n",
        "model = Transformer(src_vocab_size=len(pseudo2idx), tgt_vocab_size=len(cpp2idx)).to(device)\n",
        "model.load_state_dict(torch.load(\"pseudo2cpp_model.pt\"))\n",
        "model.eval()\n",
        "\n",
        "# Translate function\n",
        "def translate(pseudo_program):\n",
        "    with torch.no_grad():\n",
        "        src = text_to_tensor(pseudo_program, pseudo2idx, tokenize).unsqueeze(0).to(device)\n",
        "        tgt = torch.tensor([cpp2idx['<sos>']], dtype=torch.long).unsqueeze(0).to(device)\n",
        "        for _ in range(200):\n",
        "            output = model(src, tgt)\n",
        "            next_token = output[:, -1, :].argmax(dim=-1).item()\n",
        "            if next_token == cpp2idx['<eos>']:\n",
        "                break\n",
        "            tgt = torch.cat([tgt, torch.tensor([[next_token]], dtype=torch.long).to(device)], dim=1)\n",
        "        translated = [idx2cpp[idx.item()] for idx in tgt[0] if idx.item() in idx2cpp]\n",
        "        cpp = ' '.join(translated[1:])\n",
        "        cpp = re.sub(r'(\\bint\\b|\\bfor\\b|\\bwhile\\b|\\bif\\b|\\belse\\b|\\{|\\})', r'\\n\\1', cpp)\n",
        "        lines = cpp.split('\\n')\n",
        "        indented = []\n",
        "        indent_level = 0\n",
        "        for line in lines:\n",
        "            line = line.strip()\n",
        "            if line:\n",
        "                if '}' in line:\n",
        "                    indent_level = max(0, indent_level - 1)\n",
        "                indented.append('  ' * indent_level + line)\n",
        "                if '{' in line and '}' not in line:\n",
        "                    indent_level += 1\n",
        "        return '\\n'.join(indented).strip()\n",
        "\n",
        "# Gradio interface\n",
        "interface = gr.Interface(\n",
        "    fn=translate,\n",
        "    inputs=gr.Textbox(lines=10, placeholder=\"Enter pseudocode (e.g., declare gcd(a, b)...\"),\n",
        "    outputs=gr.Textbox(lines=10),\n",
        "    title=\"Pseudocode to C++ Translator\",\n",
        "    description=\"Converts multi-line pseudocode to C++ using a pre-trained Transformer.\"\n",
        ")\n",
        "interface.launch()  # No share=True, for HF Spaces\n",
        "print(\"Chunk 4 completed: Gradio deployed with loaded model.\")"
      ],
      "metadata": {
        "id": "nhlo3KEccCW7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 837
        },
        "outputId": "b1ce201f-90ff-40d0-c089-be80bd940b34"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-4a99d5e58e2c>:8: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  pseudo2idx = torch.load(\"pseudo2idx.pt\")\n",
            "<ipython-input-12-4a99d5e58e2c>:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  idx2pseudo = torch.load(\"idx2pseudo.pt\")\n",
            "<ipython-input-12-4a99d5e58e2c>:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  cpp2idx = torch.load(\"cpp2idx.pt\")\n",
            "<ipython-input-12-4a99d5e58e2c>:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  idx2cpp = torch.load(\"idx2cpp.pt\")\n",
            "<ipython-input-12-4a99d5e58e2c>:142: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"pseudo2cpp_model.pt\"))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://7766f489a6a2e40f6e.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://7766f489a6a2e40f6e.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Chunk 4 completed: Gradio deployed with loaded model.\n"
          ]
        }
      ]
    }
  ]
}